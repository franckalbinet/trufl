[
  {
    "objectID": "reader.html",
    "href": "reader.html",
    "title": "Reader",
    "section": "",
    "text": "source\n\nread_shapefile\n\n read_shapefile (shp)\n\n\nsource\n\n\nread_geojson\n\n read_geojson (shp)",
    "crumbs": [
      "Source code",
      "Reader"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utilities",
    "section": "",
    "text": "source\n\nreproject_raster\n\n reproject_raster (src_fname:str, dst_fname:str, dst_crs:str='EPSG:4326')\n\nReproject a GeoTiff file to specified crs\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_fname\nstr\n\nSource raster geotiff file\n\n\ndst_fname\nstr\n\nDestination raster geotiff file\n\n\ndst_crs\nstr\nEPSG:4326\nEPSG code to project to\n\n\nReturns\nNone\n\n\n\n\n\n\n\nExported source\ndef reproject_raster(src_fname:str, # Source raster geotiff file\n                     dst_fname:str, # Destination raster geotiff file\n                     dst_crs:str='EPSG:4326', # EPSG code to project to\n                     ) -&gt; None:\n    \"Reproject a GeoTiff file to specified crs\"\n    with rasterio.open(src_fname) as src:\n        transform, width, height = calculate_default_transform(\n            src.crs, dst_crs, src.width, src.height, *src.bounds)\n        kwargs = src.meta.copy()\n        kwargs.update({\n            'crs': dst_crs,\n            'transform': transform,\n            'width': width,\n            'height': height\n        })\n\n        with rasterio.open(dst_fname, 'w', **kwargs) as dst:\n            for i in range(1, src.count + 1):\n                reproject(\n                    source=rasterio.band(src, i),\n                    destination=rasterio.band(dst, i),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=transform,\n                    dst_crs=dst_crs,\n                    resampling=Resampling.nearest)\n\n\n\nsource\n\n\ngridder\n\n gridder (fname_raster:str, band:int=1, nrows:int=10, ncols:int=10)\n\nGenerate a grid of polygons overlaid on a raster file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname_raster\nstr\n\nThe path to the raster file.\n\n\nband\nint\n1\nThe band number to use. Defaults to 1.\n\n\nnrows\nint\n10\nThe number of rows in the grid. Defaults to 10.\n\n\nncols\nint\n10\nThe number of columns in the grid. Defaults to 10.\n\n\nReturns\nGeoDataFrame\n\nA GeoDataFrame of the grid cells geometry with ‘loc_id’ as index.\n\n\n\n\n\nExported source\ndef gridder(\n    fname_raster:str, # The path to the raster file.\n    band:int=1, # The band number to use. Defaults to 1.\n    nrows:int=10, # The number of rows in the grid. Defaults to 10.\n    ncols:int=10, # The number of columns in the grid. Defaults to 10.\n    ) -&gt; gpd.GeoDataFrame: # A GeoDataFrame of the grid cells geometry with 'loc_id' as index.\n    \"Generate a grid of polygons overlaid on a raster file.\"\n    with rasterio.open(fname_raster) as f:\n        raster = f.read(band)\n        bounds = f.bounds\n        crs = f.crs.to_string()\n\n    polygon = box(*bounds)\n    gdf_polygon = gpd.GeoDataFrame([1], geometry=[polygon], crs=crs)\n\n    # Calculate the bounds of the polygon\n    minx, miny, maxx, maxy = polygon.bounds\n\n    # Calculate the width and height of each cell\n    cell_width = (maxx - minx) / ncols\n    cell_height = (maxy - miny) / nrows\n\n    # Create an empty list to hold the grid cells\n    grid_cells = []\n\n    # Generate the grid\n    for i in range(ncols):\n        for j in range(nrows):\n            # Calculate the coordinates of the cell\n            x1 = minx + i * cell_width\n            y1 = miny + j * cell_height\n            x2 = x1 + cell_width\n            y2 = y1 + cell_height\n\n            # Create a box for each cell\n            cell = box(x1, y1, x2, y2)\n            grid_cells.append(cell)\n\n    # Create a GeoDataFrame from the grid cells\n    grid = gpd.GeoDataFrame(grid_cells, columns=['geometry'], crs=crs)\n\n    # Intersect the grid with the polygon\n    gdf = gpd.overlay(grid, gdf_polygon, how='intersection').reset_index()\n    gdf = gdf.drop(0, axis=1).set_index('index')\n    gdf.index.name = 'loc_id'\n    return gdf\n\n\n\nfname_raster = './files/ground-truth-02-4326-simulated.tif'\nprint(gridder(fname_raster).head())\nax = gridder(fname_raster, nrows=10, ncols=10).boundary.plot(lw=0.5)\nax.axis('off');\n\n                                                 geometry\nloc_id                                                   \n0       POLYGON ((-1.17017 43.02904, -1.17017 42.99324...\n1       POLYGON ((-1.17017 43.06483, -1.17017 43.02904...\n2       POLYGON ((-1.17017 43.10063, -1.17017 43.06483...\n3       POLYGON ((-1.17017 43.13643, -1.17017 43.10063...\n4       POLYGON ((-1.17017 43.17222, -1.17017 43.13643...\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nanonymize_raster\n\n anonymize_raster (fname_raster:str, new_lon_origin:float,\n                   new_lat_origin:float, band:int=1)\n\nAnonymze a raster by translating it to specified location and values standardized.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname_raster\nstr\n\nThe path to the raster file.\n\n\nnew_lon_origin\nfloat\n\nLongitude of the new origin\n\n\nnew_lat_origin\nfloat\n\nLatitude of the new origin\n\n\nband\nint\n1\nThe band number to use. Defaults to 1.\n\n\nReturns\nNone\n\n\n\n\n\n\n\nExported source\ndef anonymize_raster(fname_raster:str, # The path to the raster file.\n                     new_lon_origin:float, # Longitude of the new origin\n                     new_lat_origin:float, # Latitude of the new origin\n                     band:int=1, # The band number to use. Defaults to 1.\n                     ) -&gt; None:\n    \"Anonymze a raster by translating it to specified location and values standardized.\"\n    with rasterio.open(src_fname) as src:\n        # Calculate the new transform based on the new origin and the same resolution\n        new_transform = from_origin(new_lon_origin, new_lat_origin, src.res[0], src.res[1])\n        profile = src.profile.copy()\n        profile['transform'] = new_transform\n        \n        # Normalize values\n        array = src.read(band)\n        normalized_array = (array - array.min()) / (array.max() - array.min())\n        \n        with rasterio.open(dst_fname, 'w', **profile) as dst:\n            dst.write(normalized_array, band)",
    "crumbs": [
      "Source code",
      "Utilities"
    ]
  },
  {
    "objectID": "callbacks.html",
    "href": "callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "source",
    "crumbs": [
      "Source code",
      "Callbacks"
    ]
  },
  {
    "objectID": "callbacks.html#state",
    "href": "callbacks.html#state",
    "title": "Callbacks",
    "section": "State",
    "text": "State\n\nsource\n\nState\n\n State (measurements:geopandas.geodataframe.GeoDataFrame,\n        smp_areas:geopandas.geodataframe.GeoDataFrame,\n        cbs:List[collections.abc.Callable])\n\nCollect various variables/metrics per grid cell/administrative unit.\n\n\n\n\nType\nDetails\n\n\n\n\nmeasurements\nGeoDataFrame\nMeasurements data with loc_id, geometry and value columns.\n\n\nsmp_areas\nGeoDataFrame\nGrid of areas/polygons of interest with loc_id and geometry.\n\n\ncbs\nList\nList of Callback functions returning Variables.\n\n\n\n\n\nExported source\nclass State:\n    def __init__(self, \n                 measurements:gpd.GeoDataFrame, # Measurements data with `loc_id`, `geometry` and `value` columns. \n                 smp_areas:gpd.GeoDataFrame, # Grid of areas/polygons of interest with `loc_id` and `geometry`.\n                 cbs:List[Callable], # List of Callback functions returning `Variable`s.\n                ): \n        \"Collect various variables/metrics per grid cell/administrative unit.\"\n        fc.store_attr()\n        self.unsampled_locs = self.smp_areas.index.difference(self.measurements.index)\n\n\n\nsource\n\n\nState.get\n\n State.get (loc_id:str, as_numpy=False)\n\nGet the state variables as defined by cbs for a given location (loc_id).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nloc_id\nstr\n\nUnique id of the Point feature\n\n\nas_numpy\nbool\nFalse\nWhether or not to return a list of Variable or a tuple of numpy arrays.\n\n\n\n\n\nExported source\n@patch\ndef get(self:State, \n        loc_id:str, # Unique id of the Point feature\n        as_numpy=False # Whether or not to return a list of `Variable` or a tuple of numpy arrays.\n       ):\n    \"Get the state variables as defined by `cbs` for a given location (`loc_id`).\"\n    variables = self.run_cbs(loc_id)\n    if as_numpy:\n        return (np.array([v.name for v in variables]), \n                np.array([v.value for v in variables]))\n    else:\n        return variables\n\n\n\nsource\n\n\nState.__call__\n\n State.__call__ (loc_id=None, **kwargs)\n\nGet the state variables as defined by cbs for all loc_ids as a dataframe.\n\n\nExported source\n@patch\ndef __call__(self:State, loc_id=None, **kwargs):\n    \"Get the state variables as defined by `cbs` for all `loc_id`s as a dataframe.\"\n    loc_ids = self.smp_areas.index\n    results = [{v.name: v.value for v in self.run_cbs(loc_id)} | {'loc_id': loc_id} for loc_id in loc_ids]\n    return pd.DataFrame(results).set_index('loc_id')\n\n\n\nsource\n\n\nState.expand_to_k_nearest\n\n State.expand_to_k_nearest\n                            (subset_measurements:geopandas.geodataframe.Ge\n                            oDataFrame, k:int=5)\n\nExpand measurements of concern possibly to nearest neighbors of surrounding grid cells.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubset_measurements\nGeoDataFrame\n\nMeasurements for which Variables are computed.\n\n\nk\nint\n5\nNumber of nearest neighbours (possibly belonging to adjacent cells/admin. units to consider).\n\n\n\n\n\nExported source\n@patch\ndef expand_to_k_nearest(self:State, \n                        subset_measurements:gpd.GeoDataFrame, # Measurements for which Variables are computed.\n                        k:int=5, # Number of nearest neighbours (possibly belonging to adjacent cells/admin. units to consider).\n                       ):\n    \"Expand measurements of concern possibly to nearest neighbors of surrounding grid cells.\"\n    tree = KDTree(self.measurements.geometry.apply(lambda p: (p.x, p.y)).tolist());\n    _, indices = tree.query(subset_measurements.geometry.apply(lambda p: (p.x, p.y)).tolist(), k=k)\n    return self.measurements.iloc[indices.flatten()].reset_index(drop=True)\n\n\n\nsource\n\n\nState.run_cbs\n\n State.run_cbs (loc_id)\n\nRun Callbacks sequentially and flatten the results if required.\n\n\nExported source\n@patch\ndef _flatten(self:State, variables):\n    \"Flatten list of variables potentially containing both scalar and tuples.\"\n    return list(itertools.chain(*(v if isinstance(v, tuple) else (v,) \n                                  for v in variables)))\n\n\n\n\nExported source\n@patch\ndef run_cbs(self:State, loc_id):\n    \"Run Callbacks sequentially and flatten the results if required.\"\n    variables = []\n    for cb in self.cbs:\n        variables.append(cb(loc_id, self))\n    return self._flatten(variables)",
    "crumbs": [
      "Source code",
      "Callbacks"
    ]
  },
  {
    "objectID": "callbacks.html#callbacks",
    "href": "callbacks.html#callbacks",
    "title": "Callbacks",
    "section": "Callbacks",
    "text": "Callbacks\n\nsource\n\nMaxCB\n\n MaxCB (name='Max')\n\nCompute Maximum value of measurements at given location.\n\n\nExported source\nclass MaxCB(Callback):\n    \"Compute Maximum value of measurements at given location.\"\n    def __init__(self, name='Max'): fc.store_attr()\n    def __call__(self, \n                 loc_id:int, # Unique id of an individual area of interest. \n                 o:Type[State] # A State's object\n                ): \n        if loc_id in o.unsampled_locs: return Variable(self.name, np.nan)\n        return Variable(self.name, \n                        np.max(o.measurements.loc[[loc_id]].value.values))\n\n\n\nsource\n\n\nMinCB\n\n MinCB (name='Min')\n\nCompute Minimum value of measurements at given location.\n\n\nExported source\nclass MinCB(Callback):\n    \"Compute Minimum value of measurements at given location.\"\n    def __init__(self, name='Min'): fc.store_attr()\n    def __call__(self, \n                 loc_id:int, # Unique id of an individual area of interest. \n                 o:Type[State] # A State's object\n                ): \n        if loc_id in o.unsampled_locs: return Variable(self.name, np.nan)\n        return Variable(self.name, \n                    np.min(o.measurements.loc[[loc_id]].value.values))\n\n\n\nsource\n\n\nStdCB\n\n StdCB (name='Standard Deviation')\n\nCompute Standard deviation of measurements at given location.\n\n\nExported source\nclass StdCB(Callback):\n    \"Compute Standard deviation of measurements at given location.\"\n    def __init__(self, name='Standard Deviation'): fc.store_attr()\n    def __call__(self, \n                 loc_id:int, # Unique id of an individual area of interest. \n                 o:Type[State] # A State's object\n                ): \n        if loc_id in o.unsampled_locs: return Variable(self.name, np.nan)\n        return Variable(self.name, \n                    np.std(o.measurements.loc[[loc_id]].value.values))\n\n\n\nsource\n\n\nCountCB\n\n CountCB (name='Count')\n\nCompute the number of measurements at given location.\n\n\nExported source\nclass CountCB(Callback):\n    \"Compute the number of measurements at given location.\"\n    def __init__(self, name='Count'): fc.store_attr()\n    def __call__(self, \n                 loc_id:int, # Unique id of an individual area of interest. \n                 o:Type[State] # A State's object\n                ): \n        if loc_id in o.unsampled_locs: return Variable(self.name, np.nan)\n        return Variable(self.name, \n                        len(o.measurements.loc[[loc_id]].value.values))\n\n\n\nsource\n\n\nMoranICB\n\n MoranICB (k=5, p_threshold=0.05, name='Moran.I', min_n=5)\n\nCompute Moran.I of measurements at given location. Return NaN if p_sim above threshold.\n\n\nExported source\nclass MoranICB(Callback):\n    \"Compute Moran.I of measurements at given location. Return NaN if p_sim above threshold.\"\n    def __init__(self, k=5, p_threshold=0.05, name='Moran.I', min_n=5): fc.store_attr()\n\n    def _weights(self, measurements):\n        w = weights.KNN.from_dataframe(measurements, k=self.k)\n        w.transform = \"R\" # Row-standardization\n        return w\n        \n    def __call__(self, \n                 loc_id:int, # Unique id of an individual area of interest. \n                 o:Type[State] # A State's object\n                ): \n        if loc_id in o.unsampled_locs: return Variable(self.name, np.nan)\n        subset = o.measurements.loc[[loc_id]]\n        if len(subset) &lt;= self.min_n: return Variable(self.name, np.nan)\n        expanded_measurements = o.expand_to_k_nearest(subset, k=self.k)\n        moran = esda.moran.Moran(expanded_measurements['value'], self._weights(expanded_measurements))\n        return Variable(self.name, moran.I if moran.p_sim &lt; self.p_threshold else np.nan)\n\n\n\nsource\n\n\nPriorCB\n\n PriorCB (fname_raster:str, name:str='Prior')\n\nEmulate a prior by taking the mean of measurement over a single grid cell.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname_raster\nstr\n\nName of raster file\n\n\nname\nstr\nPrior\nName of the State variable\n\n\n\n\n\nExported source\nclass PriorCB(Callback):\n    \"Emulate a prior by taking the mean of measurement over a single grid cell.\"\n    def __init__(self, \n                 fname_raster:str, # Name of raster file\n                 name:str='Prior' # Name of the State variable\n                ): \n        fc.store_attr()\n\n    def __call__(self, \n                 loc_id:int, # Unique id of an individual area of interest. \n                 o:Type[State] # A State's object\n                ): \n        polygon = o.smp_areas.loc[o.smp_areas.reset_index().loc_id == loc_id].geometry\n        with rasterio.open(self.fname_raster) as src:\n            out_image, out_transform = mask(src, polygon, crop=True)\n            mean_value = np.mean(out_image)\n        return Variable(self.name, mean_value)\n\n\nFor example:\n\nWe generate a grid from a given raster file:\n\n\nfname_raster = 'files/ground-truth-01-4326-simulated.tif'\ngdf_grid = gridder(fname_raster, nrows=10, ncols=10)\n\n\nThen, we can emulate a data collection campaign:\n\n\n# Define random samples where to collect data\nsampler = Sampler(gdf_grid)\nnp.random.seed(41)\nn = np.random.randint(0, high=10, size=len(gdf_grid), dtype=int)\nsample_locs = sampler.sample(n, method='uniform')\n\n\n# Emulate data collection\ndc_emulator = DataCollector(fname_raster)\nsamples_t0 = dc_emulator.collect(sample_locs)\n\nax = samples_t0.plot(column='value', s=3, legend=True)\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax)\nax.axis('off');\n\n\n\n\n\n\n\n\n\nFinally, we specify the variables of current State and return it:\n\n\nstate = State(samples_t0, gdf_grid, cbs=[\n    MaxCB(), MinCB(), StdCB(), CountCB(), MoranICB(k=5), PriorCB(fname_raster)\n])\n\n# You have to call the instance\nstate_t0 = state(); state_t0\n\n\n\n\n\n\n\n\nMax\nMin\nStandard Deviation\nCount\nMoran.I\nPrior\n\n\nloc_id\n\n\n\n\n\n\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.102492\n\n\n1\n0.147935\n0.133871\n0.005889\n3.0\nNaN\n0.125727\n\n\n2\n0.173122\n0.158602\n0.007260\n2.0\nNaN\n0.161802\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\n0.184432\n\n\n4\n0.213377\n0.213377\n0.000000\n1.0\nNaN\n0.201405\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n0.857352\n0.838420\n0.009466\n2.0\nNaN\n0.803670\n\n\n96\n0.812587\n0.812587\n0.000000\n1.0\nNaN\n0.763408\n\n\n97\nNaN\nNaN\nNaN\nNaN\nNaN\n0.727797\n\n\n98\n0.703173\n0.703173\n0.000000\n1.0\nNaN\n0.646002\n\n\n99\n0.693839\n0.693839\n0.000000\n1.0\nNaN\n0.655185\n\n\n\n\n100 rows × 6 columns\n\n\n\nWhy the expand_to_k_nearest method?\nThe Moran.I index is a statistical method used to determine if there is a significant spatial autocorrelation in a dataset. It helps to characterize the level of spatial correlation within each area of interest, such as a cell in your example.\nHowever, when analyzing spatial data, it’s important to consider not just the correlation within each area, but also the correlation between areas that are close to each other. In other words, we need to take into account points that are “nearby” but “belong” to different areas.\nThe expand_to_k_nearest method is a technique that can help with this. It takes a list of points and expands it to include the nearest k-neighbors. This can be useful for identifying spatial patterns that might not be apparent when only considering the points within each area in isolation.\nBy combining the Moran.I index with the expand_to_k_nearest method, you can gain a more comprehensive understanding of the spatial patterns in your data, taking into account both the correlation within each area and the correlation between nearby areas.\nThe cell below allows to visualize it in action:\n\n# Select measurements belonging to loc_id=1\nsubset = state.measurements.loc[[1]]; subset.head()\n# Expand to k-nearest neighbors\nexpanded_pts = state.expand_to_k_nearest(subset, k=5)\n\n# Look how the k-nearest neighbors from adjacent areas are now incuded\nax = expanded_pts.plot(column='value', s=10, legend=True)\ngpd.sjoin(gdf_grid, expanded_pts, how=\"inner\", predicate='contains')\\\n    .boundary.plot(color=black, lw=0.5, ax=ax)\nax.axis('off');",
    "crumbs": [
      "Source code",
      "Callbacks"
    ]
  },
  {
    "objectID": "sampler.html",
    "href": "sampler.html",
    "title": "Sampler",
    "section": "",
    "text": "source\n\nSampler\n\n Sampler (smp_areas:geopandas.geodataframe.GeoDataFrame)\n\nSample random location in smp_areas.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsmp_areas\nGeoDataFrame\nGeographical area to sample from.\n\n\nReturns\nGeoDataFrame\nloc_id, geometry (Point or MultiPoint).\n\n\n\n\n\nExported source\nclass Sampler:\n    \"Sample random location in `smp_areas`.\"\n    def __init__(self, \n                 smp_areas:gpd.GeoDataFrame, # Geographical area to sample from.\n                ) -&gt; gpd.GeoDataFrame: # loc_id, geometry (Point or MultiPoint).\n        fc.store_attr()\n        \n    @property\n    def loc_ids(self):\n        arr = self.smp_areas.reset_index().loc_id.values\n        if len(arr) != len(np.unique(arr)):\n            raise ValueError(f'{self.loc_id_col} column contains non-unique values.')\n        else:\n            return arr\n        \n    def sample(self, \n               n:np.ndarray, # Number of samples\n               **kwargs\n              ):\n        mask = n == 0    \n        pts_gseries = self.smp_areas[~mask].sample_points(n[~mask], **kwargs)\n        gdf_pts = gpd.GeoDataFrame(geometry=pts_gseries, index=pts_gseries.index)\n        gdf_pts.index.name = 'loc_id'\n        return gdf_pts\n\n\nHow to use:\n\nfname_raster = './files/ground-truth-02-4326-simulated.tif'\ngdf_grid = gridder(fname_raster, nrows=10, ncols=10)\n\nsampler = Sampler(gdf_grid)\nn = np.random.randint(0, high=10, size=len(gdf_grid), dtype=int)\ngdf_samples = sampler.sample(n, method='uniform'); print(gdf_samples.head())\n\n                                                 geometry\nloc_id                                                   \n0       MULTIPOINT (-1.21319 43.01176, -1.20744 43.005...\n1       MULTIPOINT (-1.22320 43.03429, -1.21657 43.051...\n2       MULTIPOINT (-1.22229 43.09713, -1.21626 43.078...\n3       MULTIPOINT (-1.22233 43.12831, -1.22226 43.101...\n4       MULTIPOINT (-1.21914 43.16511, -1.21857 43.168...\n\n\n\nax = gdf_samples.plot(markersize=1, c=red)\ngdf_grid.boundary.plot(ax=ax, color=black, lw=0.5)\nax.axis('off');\n\n\n\n\n\n\n\n\n\nsource\n\n\nrank_to_sample\n\n rank_to_sample (ranks:numpy.ndarray, budget:int, min:int=0,\n                 policy:str='Weighted')\n\nMap ranks to number of samples to be collected\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nranks\nndarray\n\nRanks sorted by loc_ids\n\n\nbudget\nint\n\nTotal data collection budget available\n\n\nmin\nint\n0\nMinimum of samples to be collected per area of interest\n\n\npolicy\nstr\nWeighted\npolicy used form mapping ranks to number of samples\n\n\nReturns\nndarray\n\nNumber of samples per area of interest to be collected in the same order as ranks\n\n\n\n\n\nExported source\ndef rank_to_sample(ranks:np.ndarray, # Ranks sorted by `loc_id`s\n                   budget:int, # Total data collection budget available\n                   min:int=0, # Minimum of samples to be collected per area of interest\n                  policy:str=\"Weighted\" # policy used form mapping ranks to number of samples\n                  ) -&gt; np.ndarray: # Number of samples per area of interest to be collected in the same order as ranks\n    \"Map ranks to number of samples to be collected\"\n    if policy == \"Weighted\":\n        weights = 1/ranks\n        normalized_weights = np.array(weights) / np.sum(weights)\n        allocation = np.round(budget * normalized_weights).astype(int)\n        return np.where(allocation &lt; min, min, allocation)\n    \n    elif policy == \"quantiles\":\n        # 4. Sampling policy (based on 4 quantiles of rank)\n        n_quantile = len(ranks)/4\n        sampling_policy = [int(budget*0.5/n_quantile), int(budget*0.3/n_quantile), int(budget*0.20/n_quantile), 0]\n\n        # Calculate quantiles thresholds\n        quantiles_thresholds = np.quantile(ranks, [0.25, 0.5, 0.75, 1.0])\n        \n        # Assign each rank to a quantile\n        quantile_indices = np.digitize(ranks, quantiles_thresholds, right=True)\n        \n        # Map each quantile to its corresponding value in sampling_policy\n        samples_per_quantile = np.array([sampling_policy[i] for i in quantile_indices])\n        \n        # Ensure minimum samples collected per area\n        samples_per_quantile = np.where(samples_per_quantile &lt; min, min, samples_per_quantile)\n        \n        return samples_per_quantile\n    else:\n        raise ValueError(f'Policy {policy} not implemented.')",
    "crumbs": [
      "Source code",
      "Sampler"
    ]
  },
  {
    "objectID": "mcdm.html",
    "href": "mcdm.html",
    "title": "MCDM",
    "section": "",
    "text": "source\n\nis_normalized_matrix\n\n is_normalized_matrix (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn a Boolean value to indicate whether the matrix is normalized or not\n\nsource\n\n\nis_normalized_vector\n\n is_normalized_vector (w_vector:list)\n\nReturn a Boolean value to indicate whether the vector is normalized or not\n\nsource\n\n\ncheck_scoring_input\n\n check_scoring_input (z_matrix:&lt;built-infunctionarray&gt;, w_vector:list,\n                      is_benefit_z:list, s_method:str)\n\nRaise an exception if any argument is inappropriate for the corresponding scoring method\n\nsource\n\n\ncheck_weighting_input\n\n check_weighting_input (z_matrix:&lt;built-infunctionarray&gt;, c_method:str,\n                        w_method:str)\n\nRaise an exception if any argument is inappropriate for the corresponding weighting method\n\nsource\n\n\ncheck_normalization_input\n\n check_normalization_input (x_matrix:&lt;built-infunctionarray&gt;,\n                            is_benefit_x:list, n_method:str)\n\nRaise an exception if any argument is inappropriate for the corresponding normalization method\n\nsource\n\n\nabspearson\n\n abspearson (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn the absolute value of the Pearson correlation coefficients of the provided matrix.\n\nsource\n\n\ndcor\n\n dcor (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn the distance correlation coefficients of the provided matrix.\n\nsource\n\n\nsquared_dcov_matrix\n\n squared_dcov_matrix (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn the matrix of squared distance covariance between the columns of the provided matrix.\n\nsource\n\n\nsquared_dcor\n\n squared_dcor (jl_dcov2, j_dvar2, l_dvar2)\n\nReturn the squared distance correlation between the corresponding columns.\n\nsource\n\n\nsquared_dcov\n\n squared_dcov (j_func, l_func)\n\nReturn the squared distance covariance between the corresponding columns.\n\nsource\n\n\nlin_func\n\n lin_func (dmatrix)\n\nReturn the result of the linear function for the provided distance matrix.\n\nsource\n\n\ndist_matrix\n\n dist_matrix (z_vector:&lt;built-infunctionarray&gt;)\n\nReturn the Euclidean distance matrix of the provided vector.\n\nsource\n\n\npearson\n\n pearson (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn the Pearson correlation coefficients of the provided matrix.\n\nsource\n\n\ncorrelate\n\n correlate (z_matrix:&lt;built-infunctionarray&gt;, c_method:str)\n\nReturn the selected correlation coefficients of the provided matrix.\n\nsource\n\n\nem\n\n em (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn the weight vector of the provided decision matrix using the Entropy Measure method.\n\nsource\n\n\nmw\n\n mw (z_matrix:&lt;built-infunctionarray&gt;)\n\nReturn the weight vector of the provided decision matrix using the Mean Weights method.\n\nsource\n\n\nsd\n\n sd (z_matrix)\n\nReturn the weight vector of the provided decision matrix using the Standard Deviation method.\n\nsource\n\n\nvic\n\n vic (z_matrix:&lt;built-infunctionarray&gt;, c_method:str='dCor')\n\nReturn the weight vector of the provided decision matrix using the Variability and Interdependencies of Criteria method.\n\nsource\n\n\nlinear1\n\n linear1 (x_matrix:&lt;built-infunctionarray&gt;, is_benefit_x:list)\n\nReturn the normalized version of the provided matrix using the Linear Normalization (1) method.\n\nsource\n\n\nlinear2\n\n linear2 (x_matrix:&lt;built-infunctionarray&gt;, is_benefit_x:list)\n\nReturn the normalized version of the provided matrix using the Linear Normalization (2) method.\n\nsource\n\n\nlinear3\n\n linear3 (x_matrix:&lt;built-infunctionarray&gt;, is_benefit_x:list)\n\nReturn the normalized version of the provided matrix using the Linear Normalization (3) method.\n\nsource\n\n\nvector\n\n vector (x_matrix:&lt;built-infunctionarray&gt;, is_benefit_x:list)\n\nReturn the normalized version of the provided matrix using the Vector Normalization method.\n\nsource\n\n\nnormalize\n\n normalize (x_matrix:&lt;built-infunctionarray&gt;, is_benefit_x:list,\n            n_method:str)\n\nReturn the normalized version of the provided matrix using the selected normalization method.\n\nsource\n\n\ncritic\n\n critic (z_matrix:&lt;built-infunctionarray&gt;, c_method:str='Pearson')\n\nReturn the weight vector of the provided decision matrix using the Criteria Importance Through Intercriteria Correlation method.\n\nsource\n\n\nweigh\n\n weigh (z_matrix:&lt;built-infunctionarray&gt;, w_method:str, c_method:str=None)\n\nReturn the weight vector of the provided decision matrix using the selected weighting method.\n\nsource\n\n\ntopsis\n\n topsis (z_matrix:&lt;built-infunctionarray&gt;, w_vector:str,\n         is_benefit_z:list)\n\nReturn the Technique for Order Preference by Similarity to Ideal Solution scores of the provided decision matrix with the provided weight vector.\n\nsource\n\n\ncp\n\n cp (z_matrix:&lt;built-infunctionarray&gt;, w_vector:list, is_benefit_z:list)\n\nReturn the Technique for Order Preference by Similarity to Ideal Solution scores of the provided decision matrix with the provided weight vector.\n\nsource\n\n\nscore\n\n score (z_matrix:&lt;built-infunctionarray&gt;, is_benefit_z:list,\n        w_vector:list, s_method:str)\n\nReturn the selected scores of the provided decision matrix with the provided weight vector.",
    "crumbs": [
      "Source code",
      "MCDM"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trufl",
    "section": "",
    "text": "Trufl was initiated in the context of the IAEA (International Atomic Energy Agency) Coordinated Research Project (CRP) titled “Monitoring and Predicting Radionuclide Uptake and Dynamics for Optimizing Remediation of Radioactive Contamination in Agriculture”.\nWhile Trufl was originally developed to address the remediation of farmland affected by nuclear accidents, its approach and algorithms are applicable to a wide range of application domains. This includes managing legacy contaminants or monitoring phenomena that require consideration of multiple decision criteria over time, taking into account a wide range of factors and contexts.\nThis package leverages the work done by Floris Abrams in the context of his PhD in collaboration between SCK CEN and KU Leuven and Franck Albinet, International Consultant in Geospatial Data Science and currently PhD researcher in AI applied to nuclear remedation at KU Leuven.",
    "crumbs": [
      "Trufl"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Trufl",
    "section": "Install",
    "text": "Install\npip install trufl",
    "crumbs": [
      "Trufl"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Trufl",
    "section": "Getting started",
    "text": "Getting started\nIn highly sensitive and high-stakes situations, it is essential that decision making is informed, transparent, and accountable, with decisions being based on a thorough and objective analysis of the available data and the needs and concerns of affected communities being taken into account.\nGiven the time constraints and limited budgets that are often associated with data surveys (in particular ones supposed to informed highly sensitive situation), it is crucial to make informed decisions about how to allocate resources. This is even more important when considering the many variables that can be taken into account, such as prior knowledge of the area, health and economic impacts, land use, whether remediation has already taken place, population density, and more. Our approach leverages Multiple-criteria decision-making approaches to optimize the data survey workflow:\nIn this demo, we will walk you through a typical workflow using the Trufl package. To help illustrate the process, we will use a “toy” dataset that represents a typical spatial pattern of soil contaminants.\n\nWe assume that we have access to the ground truth, which is a raster file that shows the spatial distribution of a soil contaminant;\nWe will make decisions about how to optimally sample the administrative units (polygons), which in this case are simulated as a grid (using the gridder utilities function);\nBased on prior knowledge, such as prior airborne surveys or other data, an Optimizer will rank each administrative unit (grid cell) according to its priority for sampling;\nWe will then perform random sampling on the designated units (grid cells) (using a Sampler). To simulate the measurement process, we will use the ground truth to emulate measurements at each location (using a DataCollector);\nWe will evaluate the new state of each unit based on the measurements and pass it to a new round of optimization. This process will be repeated iteratively to refine the sampling strategy.\n\n\nImports\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\nimport numpy as np\nimport pandas as pd\nimport rasterio\nimport geopandas as gpd\n\nfrom trufl.utils import gridder\nfrom trufl.sampler import Sampler, rank_to_sample\nfrom trufl.collector import DataCollector\nfrom trufl.callbacks import (State, MaxCB, MinCB, StdCB, CountCB, MoranICB, PriorCB)\nfrom trufl.optimizer import Optimizer\n\n\nred, black = '#BF360C', '#263238'\n\n\n\nOur simulated ground truth\nThe assumed ground truth reveals a typical spatial pattern of contaminant such as Cs137 after a nuclear accident for instance:\n\nfname_raster = './files/ground-truth-01-4326-simulated.tif'\nwith rasterio.open(fname_raster) as src:\n    plt.axis('off')\n    plt.imshow(src.read(1))\n    plt.title('Simulated Ground Truth')\n\n\n\n\n\n\n\n\n\n\nSimulate administrative units\nThe sampling strategy will be determined on a per-grid-cell basis within the administrative unit. We define below a 10 x 10 grid over the area of interest:\n\ngdf_grid = gridder(fname_raster, nrows=10, ncols=10)\ngdf_grid.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\nloc_id\n\n\n\n\n\n0\nPOLYGON ((-1.20830 43.26950, -1.20830 43.26042...\n\n\n1\nPOLYGON ((-1.20830 43.27858, -1.20830 43.26950...\n\n\n2\nPOLYGON ((-1.20830 43.28766, -1.20830 43.27858...\n\n\n3\nPOLYGON ((-1.20830 43.29673, -1.20830 43.28766...\n\n\n4\nPOLYGON ((-1.20830 43.30581, -1.20830 43.29673...\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how each administrative unit is uniquely identified by its loc_id.\n\n\n\ngdf_grid.boundary.plot(color=black, lw=0.5)\nplt.axis('off')\nplt.title('Simulated Administrative Units');\n\n\n\n\n\n\n\n\n\n\nRound I: Optimize sampling based on prior at \\(t_0\\)\n\nWhat prior knowledge do we have?\nAt the initial time \\(t_0\\), data sampling has not yet begun, but we can often leverage existing prior knowledge of our phenomenon of interest to inform our sampling strategy/policy. In the context of nuclear remediation, this prior knowledge can often be obtained through mobile surveys, such as airborne or carborne surveys, which can provide a coarse estimation of soil contamination levels.\nIn the example below, we simulate prior information about the soil property of interest by calculating the average value of the property over each grid cell.\nAt this stage, we have no measurements, so we simply create an empty Geopandas GeoDataFrame.\n\nsamples_t0 = gpd.GeoDataFrame(index=pd.Index([], name='loc_id'), \n                              geometry=None, data={'value': None})\n\n\n\n\n\n\n\nTip\n\n\n\nWe need to set an index loc_id and have a geometry and value columns.\n\n\nNow we get/“sense” the state of our grid cells based on the simulated prior (Mean over each grid cell PriorCB):\n\nstate = State(samples_t0, gdf_grid, cbs=[PriorCB(fname_raster)])\n\n# You have to call the instance\nstate_t0 = state(); state_t0.head()\n\n\n\n\n\n\n\n\nPrior\n\n\nloc_id\n\n\n\n\n\n0\n0.102492\n\n\n1\n0.125727\n\n\n2\n0.161802\n\n\n3\n0.184432\n\n\n4\n0.201405\n\n\n\n\n\n\n\n\ngdf_grid.join(state_t0, how='left').plot(column='Prior',\n                                         cmap='viridis', \n                                         legend_kwds={'label': 'Value'}, \n                                         legend=True)\nplt.axis('off')\nplt.title('Prior: Mean value at Administrative Unit level');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe get the Prior for each individual loc_id (here only the first 5 shown). The current State is only composed of a single PriorCB variable but can include many more variables as we will see below.\n\n\n\n\nSampling priority ranks\n\nbenefit_criteria = [True]\noptimizer = Optimizer(state=state_t0)\ndf_rank = optimizer.get_rank(is_benefit_x=benefit_criteria, w_vector = [1],  \n                             n_method=None, c_method = None, \n                             w_method=None, s_method=\"CP\")\n\ndf_rank.head()\n\n\n\n\n\n\n\n\nrank\n\n\nloc_id\n\n\n\n\n\n92\n1\n\n\n93\n2\n\n\n91\n3\n\n\n94\n4\n\n\n82\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor more information on how the Optimizer operates, please see the section Delving deeper into the optimization process.\n\n\n\ngdf_grid.join(df_rank, how='left').plot(column='rank',\n                                        cmap='viridis_r', \n                                        legend_kwds={'label': 'Rank'}, \n                                        legend=True)\nplt.axis('off')\nplt.title('Sampling Priorirty Rank');\n\n\n\n\n\n\n\n\n\n\nInformed random sampling\n\n\n\n\n\n\nTip\n\n\n\nIt’s worth noting that in the absence of any prior knowledge, a uniform sampling strategy over the area of interest may be used. However, this approach may not be the most efficient use of the available data collection and analysis budget.\n\n\nBased on the ranks (sampling priority) calculated by the Optimizer and given sampling budget, let’s calculate the number of samples to be collected for each administrative unit (loc_id). Different sampling policies can be used (Weighted, Quantiles, …):\n\nbudget_t0 = 600\nn = rank_to_sample(df_rank['rank'].sort_index().values, \n                   budget=budget_t0, min=1, policy=\"quantiles\"); n\n\narray([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1,  4,  4,\n        4,  4,  4,  1,  1,  1,  1,  1,  4,  4,  4,  4,  4,  1,  1,  1,  4,\n        4,  7,  7, 12,  7,  7,  1,  1,  4,  4,  7, 12, 12, 12, 12,  7,  1,\n        4,  4,  7,  7, 12, 12, 12,  7,  4,  4,  4,  7,  7,  7,  7,  7,  7,\n        4,  4,  4,  7, 12,  7, 12, 12,  7,  7,  4,  4,  7, 12, 12, 12, 12,\n       12, 12, 12,  7,  7, 12, 12, 12, 12, 12, 12, 12,  7,  7,  7])\n\n\nWe can now decide where to sample based on this sampling schema:\n\nsampler = Sampler(gdf_grid)\nsample_locs_t0 = sampler.sample(n, method='uniform')\n\nprint(sample_locs_t0.head())\nax = sample_locs_t0.plot(markersize=2, color=red)\n\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax)\nplt.axis('off')\nplt.title('Ranked Random Samples Location');\n\n                         geometry\nloc_id                           \n0       POINT (-1.21727 43.26778)\n1       POINT (-1.22102 43.27806)\n2       POINT (-1.21712 43.27979)\n3       POINT (-1.22145 43.29287)\n4       POINT (-1.21036 43.30109)\n\n\n\n\n\n\n\n\n\n\n\nEmulating measurement campaign\nThe data collector collects measurements at the random sampling locations in the field. In our case, we emulate this process by extracting measurements from the provided raster file.\n“Measuring” variable of interest from a given raster:\n\ndc_emulator = DataCollector(fname_raster)\nmeasurements_t0 = dc_emulator.collect(sample_locs_t0)\n\nprint(measurements_t0.head())\nax = measurements_t0.plot(column='value', s=2, legend=True)\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax);\nplt.axis('off')\nplt.title('Measurements at Random Sampling Points');\n\n                         geometry     value\nloc_id                                     \n0       POINT (-1.21727 43.26778)  0.137188\n1       POINT (-1.22102 43.27806)  0.151005\n2       POINT (-1.21712 43.27979)  0.164272\n3       POINT (-1.22145 43.29287)  0.181001\n4       POINT (-1.21036 43.30109)  0.168969\n\n\n\n\n\n\n\n\n\nThis marks the end of our initial measurement efforts, based on our prior knowledge of the phenomenon. Going forward, we can use the additional insights gained during this phase to enhance our future measurements.\n\n\n\nRound II: Optimize sampling with additional insights at \\(t_1\\)\nFor each administrative unit, we now have additional knowledge acquired during the previous campaign, in addition to our prior knowledge. In the current round, the optimization of the sampling will be carried out based on the maximum, minimum, standard Deviation, number of measurements already conducted, our prior knowledge, and an estimate of the presence of spatial trends or spatial correlations (Moran’s I).\n\n\n\n\n\n\nTip\n\n\n\nIt’s worth noting that you can use any quantitative or qualitative secondary geographical information as a variable in the state, such as population, whether any previous remediation actions have taken place, the economic impact of the contamination, and so on.\n\n\n\nGetting administrative units new state\n\nstate = State(measurements_t0, gdf_grid, cbs=[\n    MaxCB(), MinCB(), StdCB(), CountCB(), MoranICB(k=5), PriorCB(fname_raster)])\n\n\nstate().head()\n\n\n\n\n\n\n\n\nMax\nMin\nStandard Deviation\nCount\nMoran.I\nPrior\n\n\nloc_id\n\n\n\n\n\n\n\n\n\n\n0\n0.137188\n0.137188\n0.0\n1\nNaN\n0.102492\n\n\n1\n0.151005\n0.151005\n0.0\n1\nNaN\n0.125727\n\n\n2\n0.164272\n0.164272\n0.0\n1\nNaN\n0.161802\n\n\n3\n0.181001\n0.181001\n0.0\n1\nNaN\n0.184432\n\n\n4\n0.168969\n0.168969\n0.0\n1\nNaN\n0.201405\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I index is a statistical method used to determine if there is a spatial correlation/trend within each area of interest. For example, a random field would have a Moran’s I index close to 0, while a clear gradient of low to high values, such as from south to north, would be characterized by a Moran’s I index close to 1.\n\n\n\n\nFinding optimal number of samples to be collected\n\nWe first decide if each variable of the State are to maximize (benefit) or minimize (cost):\n\n\nbenefit_criteria = [True, True, True, False, False, True]\n\n\nThen assign an importance weight to each of the variable of the State (Min, Max, …):\n\n\noptimizer = Optimizer(state=state())\ndf_rank = optimizer.get_rank(is_benefit_x=benefit_criteria, \n                             w_vector = [0.2, 0.1, 0.1, 0.2, 0.2, 0.2],  \n                             n_method=\"LINEAR1\", c_method=None, w_method=None, s_method=\"CP\")\n\n\ngdf_grid.join(df_rank, how='left').plot(column='rank',\n                                        cmap='viridis_r', \n                                        legend_kwds={'label': 'Rank'}, \n                                        legend=True)\nplt.axis('off')\nplt.title('Sampling Priorirty Rank');\n\n\n\n\n\n\n\n\n\ndf_rank.head()\n\n\n\n\n\n\n\n\nrank\n\n\nloc_id\n\n\n\n\n\n26\n1\n\n\n73\n2\n\n\n27\n3\n\n\n78\n4\n\n\n24\n5\n\n\n\n\n\n\n\nBased on this rank we can again: 1. based on the ranks (sampling priority) and given sampling budget, calculate the number of samples to be collected for each administrative unit and carry out random sampling; 2. perform the random sampling; 3. and carry out the measurements.\n\n\nInformed random sampling\n\nbudget_t1 = 400\nn = rank_to_sample(df_rank['rank'].sort_index().values, \n                   budget=budget_t1, min=1, policy=\"quantiles\"); n\n\narray([1, 1, 1, 1, 1, 1, 3, 4, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 8, 1, 1,\n       1, 1, 8, 8, 8, 8, 4, 8, 1, 1, 1, 3, 8, 4, 4, 4, 8, 3, 1, 1, 3, 4,\n       4, 4, 4, 4, 4, 3, 1, 3, 4, 4, 8, 3, 3, 4, 8, 8, 1, 8, 3, 3, 4, 4,\n       4, 3, 4, 8, 8, 8, 8, 8, 4, 3, 4, 8, 8, 8, 8, 4, 8, 8, 4, 3, 3, 3,\n       3, 3, 8, 4, 3, 3, 4, 4, 3, 8, 3, 3])\n\n\n\nsampler = Sampler(gdf_grid)\nsample_locs_t1 = sampler.sample(n, method='uniform')\n\nax = sample_locs_t1.plot(markersize=2, color=red)\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax)\nplt.axis('off')\nplt.title('Ranked Random Samples Location');\n\n\n\n\n\n\n\n\n\n\nSecond measurement campaign\n\ndc_emulator = DataCollector(fname_raster)\nmeasurements_t1 = dc_emulator.collect(sample_locs_t1)\n\nax = measurements_t1.plot(column='value', s=2, legend=True)\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax);\nplt.axis('off')\nplt.title('Measurements at Random Sampling Points');\n\n\n\n\n\n\n\n\n\nmeasurements_sofar = pd.concat([measurements_t0, measurements_t1])\n\nax = measurements_sofar.plot(column='value', s=2, legend=True)\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax);\nplt.axis('off')\nplt.title('Measurements after \\n 2 informed measurement campaigns');",
    "crumbs": [
      "Trufl"
    ]
  },
  {
    "objectID": "index.html#delving-deeper-into-the-optimization-process",
    "href": "index.html#delving-deeper-into-the-optimization-process",
    "title": "Trufl",
    "section": "Delving deeper into the optimization process",
    "text": "Delving deeper into the optimization process\n\nDetermine the ranking of the administrative polygons\nThe ranking is based on the importance of increasing sampling in each polygon. A multi-criteria decision-making methodology is used to rank the polygons from most important to least important, with lower ranks indicating a higher priority for sampling.\n\nCriteria\nThe state of the polygons will be used as criteria to determine the rank:\n\n\n\nCriteria\nState variable\nCriteria Type\n\n\n\n\n\nEstimated value\nPriorCB()\nBenefit\n\n\n\nMaximum sample value\nMaxCB()\nBenefit\n\n\n\nMinimal sample value\nMinCB()\nBenefit\n\n\n\nSample count\nCountCB()\nCost\n\n\n\nStandard deviation\nStdCB()\nBenefit\n\n\n\nMoran I index\nMoranICB(k=5)\nCost\n\n\n\n\n\n\nCriteria type\nCriteria can be of the type benefit or cost:\n\nBenefit: high values equal high importance to sample more;\nCost: low value equal high importance to sample more).\n\n\n\nWeights\nA weight vector is used to determine the importance of criteria in comparison with each other.\n\n\nMCDM techniques\n\nCP (Compromise programming):\n\nDistance based measure, where the distance to the optmal point is used, where low values relate to good alternatives.\n\nTOPSIS (Technique for Order Preference by Similarity to Ideal Solution):\n\nDistance-based measure, where the closeness to the optimal and anti-optimal points is assessed (with higher values indicating better alternatives).\n\n\n\n\nRank\nBased on the MCDM value a ranking of the polygons is created:\n\n\n\n\n\n\nTip\n\n\n\nStart with using equal weights for all the criteria, later you will explore the impact of changing the weight vector. Make sure the sum of the weight vector is 1.\n\n\nRanking of administrative units based on three criteria:\n\nbenefit_criteria = [True, True, True]\nstate = State(measurements_sofar, gdf_grid, cbs=[MaxCB(), MinCB(), StdCB()])\nweight_vector = [0.3, 0.3, 0.4]\n\noptimizer = Optimizer(state=state())\ndf = optimizer.get_rank(is_benefit_x=benefit_criteria, w_vector = weight_vector,  \n                    n_method=\"LINEAR1\", c_method = None, w_method=None, s_method=\"CP\")\n\ndf.head()\n\n\n\n\n\n\n\n\nrank\n\n\nloc_id\n\n\n\n\n\n71\n1\n\n\n72\n2\n\n\n69\n3\n\n\n59\n4\n\n\n38\n5\n\n\n\n\n\n\n\nBased on the ranking of the administrative units, an optimized sampling strategy for \\(t_1\\) can be determined.\n\ncombined_df = pd.merge(df, gdf_grid[['geometry']], left_index=True, right_index=True, how='inner')\ncombined_gdf = gpd.GeoDataFrame(combined_df)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\ncax = combined_gdf.plot(column='rank', cmap='Reds_r', legend=True, ax=ax)\nmeasurements_sofar.plot(column='value', ax=ax, cmap='viridis', s=1.5, legend=True)\n\ncbar = cax.get_figure().get_axes()[1]\ncbar.invert_yaxis()\n\nrank_legend = mlines.Line2D([], [], color='Red', marker='o', linestyle='None',\n                            markersize=10, label='High Rank')\nvalue_legend = mlines.Line2D([], [], color='Yellow', marker='o', linestyle='None',\n                             markersize=10, label='High prior value')\n\nax.legend(handles=[rank_legend, value_legend], loc='upper left', bbox_to_anchor=(1.5, 1.25))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMulti-year Adaptive sampling approach\n\nSampling in year 0 will done based on the prior;\nSampling in year t will be done based on 6 state variables:\n\n[Max value, Min value, Standard deviation, sample count, Moran I, Prior value]\n[0.2, 0.1, 0.1, 0.2, 0.2, 0.2]\n\nSampling policy will be based on the point budget and the quantile in which the unit ranks:\n\n1st: 50 % of point budget\n2nd: 30% of point budget\n3th: 20% of point budget\n4th: no extra sample points\n\n\n\nnumber_of_years = 4\nyearly_sample_budget = 150\n\n\nfig, axs = plt.subplots(1, number_of_years, figsize=(12, 8))  # Adjust figsize as needed\naxs = axs.flatten()\n\nsampler = Sampler(gdf_grid)\ndc_emulator = DataCollector(fname_raster)\n\n# Samples\nsamples_t_0 = gpd.GeoDataFrame(index=pd.Index([], name='loc_id'), geometry=None, data={'value': None})\nsamples_t = []\n\nstate = State(samples_t_0, gdf_grid, cbs=[PriorCB(fname_raster)])\n\n# You have to call the instance\nstate_t0 = state()\n\nbenefit_criteria = [True]\noptimizer = Optimizer(state=state_t0)\ndf_rank = optimizer.get_rank(is_benefit_x=benefit_criteria, w_vector = [1],  \n                             n_method=None, c_method = None, \n                             w_method=None, s_method=\"CP\")\n\ncombined_df = pd.merge(df, gdf_grid[['geometry']], left_index=True, right_index=True, how='inner')\ncombined_gdf = gpd.GeoDataFrame(combined_df)\ncombined_gdf.plot(column='rank',cmap='Reds_r', legend_kwds={'label': 'Rank'}, ax = axs[0])\n\nfor fig_n, ax in zip(range(1, number_of_years+1), axs[1:]):\n    n = rank_to_sample(combined_gdf['rank'].sort_index().values, \n                    budget=yearly_sample_budget, min=1, policy=\"quantiles\")\n    sample_locs_t = sampler.sample(n, method='uniform')\n    samples = dc_emulator.collect(sample_locs_t)\n    try:\n        samples_t = pd.concat([samples_t, samples])\n    except:\n        samples_t = pd.concat([samples])\n    \n    # plot points versus rank of polygon\n    ax = combined_gdf.plot(column='rank', cmap='Reds_r', ax=ax)\n    samples_t.plot(column='value', ax=ax, cmap='viridis', s=1)\n    ax.title.set_text(f\"Year {fig_n} (number of samples: {len(samples_t)})\")\n    \n    # new state\n    state = State(samples_t, gdf_grid, cbs=[\n        MaxCB(), MinCB(), StdCB(), CountCB(), MoranICB(k=5), PriorCB(fname_raster)])\n    \n    optimizer = Optimizer(state=state())\n\n    # 2. rank polygons\n    benefit_criteria = [True, True, True, False, False, True]\n    df = optimizer.get_rank(is_benefit_x=benefit_criteria, w_vector = [0.2, 0.1, 0.1, 0.2, 0.2, 0.2],  n_method=\"LINEAR1\", c_method = None, w_method=None, s_method=\"CP\")\n\n    # 3. map ranking\n    combined_df = pd.merge(df, gdf_grid[['geometry']], left_index=True, right_index=True, how='inner')\n    combined_gdf = gpd.GeoDataFrame(combined_df)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Trufl"
    ]
  },
  {
    "objectID": "optimizer.html",
    "href": "optimizer.html",
    "title": "Optimizer",
    "section": "",
    "text": "source\n\nOptimizer\n\n Optimizer (state:pandas.core.frame.DataFrame)\n\nOptimize the number of points for t. Provided the number of points to sample in t based on t-1, return values number of sample points.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate\nDataFrame\na dataframe with the state of the administrative boundaries\n\n\n\n\n\nExported source\nclass Optimizer:\n    def __init__(self,\n                 state:pd.DataFrame # a dataframe with the state of the administrative boundaries\n                 ):\n        \"Optimize the number of points for t. Provided the number of points to sample in t based on t-1, return values number of sample points.\"\n        self.state = state\n        \n        self.matrix = state.to_numpy()\n        return\n\n\n\nsource\n\n\nOptimizer.get_rank\n\n Optimizer.get_rank (is_benefit_x:list, w_vector:list, n_method:str=None,\n                     c_method:str=None, w_method:str=None,\n                     s_method:str=None)\n\nDetermines the rank of the administrative polygon based on the provided states.\n\n\nExported source\n@patch\ndef get_rank(self:Optimizer, \n             is_benefit_x:list,\n             w_vector:list,  \n            n_method:str=None,\n            c_method:str =None, \n            w_method:str=None,\n            s_method:str=None\n            ):\n    \"Determines the rank of the administrative polygon based on the provided states.\"\n    # normailize the matrix\n    z_matrix, is_benefit_z = normalize(self.matrix, is_benefit_x, n_method)\n    \n     # replace nan values with 0 if cost and & 1 if benefit\n    for i, is_benefit in enumerate(is_benefit_z):\n        if is_benefit:\n            z_matrix[:, i][np.isnan(z_matrix[:, i])] = 1\n        else:\n            z_matrix[:, i][np.isnan(z_matrix[:, i])] = 0\n\n    if w_vector is None:\n            # Weigh each criterion using the selected methods\n            w_vector = weigh(z_matrix, w_method, c_method)\n    else:\n        pass\n\n    s_vector, desc_order = score(z_matrix, is_benefit_z, w_vector, s_method)\n    state = self.state.copy()\n    state['value'] = s_vector\n    df = state[['value']]\n\n    # Get the indices of the sorted scores\n    if desc_order:\n        df_sorted = df.sort_values(by='value', ascending=False)\n    else:\n        df_sorted = df.sort_values(by='value', ascending=True)\n    \n    df_sorted['rank'] = range(1, len(df_sorted) + 1)\n    df_rank = df_sorted[['rank']]\n    \n    return df_rank",
    "crumbs": [
      "Source code",
      "Optimizer"
    ]
  },
  {
    "objectID": "collector.html",
    "href": "collector.html",
    "title": "Data Collector",
    "section": "",
    "text": "source\n\nDataCollector\n\n DataCollector (fname_raster:str, band:int=1)\n\nEmulate data collection. Provided a set of location, return values sampled from given raster file.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname_raster\nstr\n\nThe path to the raster file.\n\n\nband\nint\n1\nThe band number to use. Defaults to 1.\n\n\n\n\n\nExported source\nclass DataCollector:\n    def __init__(self, \n                 fname_raster:str, # The path to the raster file.\n                 band:int=1, # The band number to use. Defaults to 1.\n                ):\n        \"Emulate data collection. Provided a set of location, return values sampled from given raster file.\"\n        fc.store_attr()\n        with rasterio.open(fname_raster) as src:\n            self.band_data = src.read(band)\n            self.affine = src.transform\n            self.bounds = src.bounds\n    \n    def get_values(self, \n                   gdf:gpd.GeoDataFrame # loc_id and Point/Multipoint geometry of samples where to measure.\n                  ):\n        coords = [(x, y) for x, y in gdf.get_coordinates().values]\n        pixel_coords = [transform.rowcol(self.affine, *pair) for pair in coords]\n        return [self.band_data[int(x), int(y)] for (x, y) in pixel_coords]\n        \n    def collect(self, \n                gdf:gpd.GeoDataFrame # loc_id and Point/Multipoint geometry of samples where to measure.\n               ) -&gt; gpd.GeoDataFrame:\n        return gdf.explode(index_parts=False).assign(value=self.get_values(gdf))\n\n\nHow to use: 1. Generate a grid:\n\nfname_raster = './files/ground-truth-01-4326-simulated.tif'\ngdf_grid = gridder(fname_raster, nrows=10, ncols=10)\n\n\nGenerate sample locations where measurements should be taken:\n\n\nsampler = Sampler(gdf_grid)\nn = np.random.randint(1, high=10, size=len(gdf_grid), dtype=int)\nsample_locs = sampler.sample(n, method='uniform')\n\n\nEmulate data collection, “taking” measurements from a given raster file:\n\n\ndc_emulator = DataCollector(fname_raster)\nsamples_t0 = dc_emulator.collect(sample_locs)\nprint(samples_t0.head())\n\nax = samples_t0.plot(column='value', s=3, legend=True)\ngdf_grid.boundary.plot(color=black, lw=0.5, ax=ax)\nax.axis('off');\n\n                         geometry     value\nloc_id                                     \n0       POINT (-1.22304 43.26776)  0.000000\n0       POINT (-1.22053 43.26270)  0.143665\n0       POINT (-1.22018 43.26424)  0.146753\n0       POINT (-1.21862 43.26922)  0.138506\n0       POINT (-1.21596 43.26927)  0.127660",
    "crumbs": [
      "Source code",
      "Data Collector"
    ]
  }
]